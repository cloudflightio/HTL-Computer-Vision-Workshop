{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab6a929-6cb9-4154-b83e-3dbeb9be17e5",
   "metadata": {
    "id": "cab6a929-6cb9-4154-b83e-3dbeb9be17e5"
   },
   "source": [
    "# Image Classification\n",
    "\n",
    "In this jupyter lab notebook, we will train a neural network to classify happy and sad smilies :)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip data.zip -d data\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAGyUqhk-gxE",
    "outputId": "db14dd48-7b1e-4920-c25f-c9943608db52"
   },
   "id": "hAGyUqhk-gxE",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7e58d0a3-6ef3-49d3-b55c-7d6b372d629a",
   "metadata": {
    "id": "7e58d0a3-6ef3-49d3-b55c-7d6b372d629a"
   },
   "source": [
    "## Import\n",
    "Here we import libraries we will use in order to code our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa878271-d8d6-45a4-820c-43ee3e8e65eb",
   "metadata": {
    "id": "aa878271-d8d6-45a4-820c-43ee3e8e65eb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import pathlib\n",
    "\n",
    "# set a seed to have reproducible results\n",
    "tf.keras.utils.set_random_seed(\n",
    "    2012\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ca029-caed-4735-8ed9-3ce844d75811",
   "metadata": {
    "id": "079ca029-caed-4735-8ed9-3ce844d75811"
   },
   "source": [
    "## Prepare our dataset\n",
    "We photographed and cut out all the smilies you drew. We put all the happy smilies in a folder called \"happy\" and all the sad ones in a folder called \"sad\".\n",
    "\n",
    "Also, we took a small part of the smilies (equal amount of sad and happy ones) and moved them to another folder. These smileys will not be seen by our network and we use them only to check how well our network can already recognize new 'unknown' smilies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5b0ae-fd45-4abc-a804-d0297ce2a28d",
   "metadata": {
    "id": "a2d5b0ae-fd45-4abc-a804-d0297ce2a28d"
   },
   "outputs": [],
   "source": [
    "# the smilies we will use to train the network\n",
    "smilies_dir_train = pathlib.Path(\"data/smilies_train\")\n",
    "# the smilies we will use to evalute the network\n",
    "smilies_dir_val = pathlib.Path(\"data/smilies_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82332772-408a-46ad-8a4f-027d87bbb6e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82332772-408a-46ad-8a4f-027d87bbb6e0",
    "outputId": "1784ab1e-b7f1-481d-c42e-61c5f4046a2b"
   },
   "outputs": [],
   "source": [
    "image_count = len(list(smilies_dir_train.glob('*/*.png')))\n",
    "print(f\"We have {image_count} images loaded and ready to use :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f79a50-5285-4b9e-a4fd-9fcb4bf4aae8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "38f79a50-5285-4b9e-a4fd-9fcb4bf4aae8",
    "outputId": "ada45a54-1024-4395-a3e9-6187ee2fbb6b"
   },
   "outputs": [],
   "source": [
    "# we take a random 'happy' smiley to ensure we did everything correct\n",
    "happy = list(smilies_dir_train.glob('happy/*'))\n",
    "PIL.Image.open(str(happy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af2e9e-90c5-4c54-adb9-01cc58231654",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "51af2e9e-90c5-4c54-adb9-01cc58231654",
    "outputId": "07af3ce5-521a-4192-fe75-259c76abbc51"
   },
   "outputs": [],
   "source": [
    "# and another one\n",
    "PIL.Image.open(str(happy[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70875d82-e6cd-402f-9874-e561a2b2c994",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "70875d82-e6cd-402f-9874-e561a2b2c994",
    "outputId": "b6d32f99-815c-4164-df6d-e85b978d0e66"
   },
   "outputs": [],
   "source": [
    "# we do the same for the sad smileys to be extra sure\n",
    "sad = list(smilies_dir_train.glob('sad/*'))\n",
    "PIL.Image.open(str(sad[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0621ff95-9210-4b9e-91e7-931cf9fd9e57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "0621ff95-9210-4b9e-91e7-931cf9fd9e57",
    "outputId": "69f69c6a-4f16-4bb0-ecc3-f08a4b1ab884"
   },
   "outputs": [],
   "source": [
    "# and another one\n",
    "PIL.Image.open(str(sad[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10250c85-5888-4271-9d6e-c3e4c4d8d1ba",
   "metadata": {
    "id": "10250c85-5888-4271-9d6e-c3e4c4d8d1ba"
   },
   "source": [
    "## Create a tensorflow dataset\n",
    "\n",
    "Our images are now ready and stored in the named folders. Keras allows us to simply load this folder into our program. Whether it is a happy or sad smiley, Keras recognizes it by the folder name.\n",
    "\n",
    "The images are loaded into a so-called dataset. From there we can easily access them to train the model or to further process the images.\n",
    "\n",
    "When working with neural networks, it is common to show the network several images at the same time in one step. This helps it to learn features that appear in several images and can also shorten the training time.\n",
    "How many such images the network sees in one step is configured by the batch size. Usually one takes a large number, even several hundred images. Since our dataset is not that large today, we will work with a batch size of 4.\n",
    "\n",
    "Today our network can only work with images of the same size, but since we cut them out ourselves, by hand, it is also important to bring them to a uniform size. Here we use 64x64 pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6d1e2-7fd6-40ff-ae18-24464c11b860",
   "metadata": {
    "id": "aff6d1e2-7fd6-40ff-ae18-24464c11b860"
   },
   "outputs": [],
   "source": [
    "# Set some hyperparameters\n",
    "batch_size = 4\n",
    "img_height = 64\n",
    "img_width = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03faedfc-fe6a-40bf-9a3b-4135e04fe381",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03faedfc-fe6a-40bf-9a3b-4135e04fe381",
    "outputId": "31a754e0-8420-4f24-9e8a-f6776aafd0fd"
   },
   "outputs": [],
   "source": [
    "# load the data in our train dataset\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    smilies_dir_train,\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc0043-22d3-4d93-a01c-ac1e902404d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3dc0043-22d3-4d93-a01c-ac1e902404d9",
    "outputId": "0d0822ad-5a0a-488f-e002-6430e6037611"
   },
   "outputs": [],
   "source": [
    "# load the data in our validation dataset\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    smilies_dir_val,\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91e905-ed10-44b0-ae0e-3cf6f0a1925f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf91e905-ed10-44b0-ae0e-3cf6f0a1925f",
    "outputId": "50c9b882-ffdb-45f0-b2cb-b5f2ff4cdcb4"
   },
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4e9259-e25a-4852-ac10-82576203f6f4",
   "metadata": {
    "id": "9f4e9259-e25a-4852-ac10-82576203f6f4"
   },
   "source": [
    "We take a look at the samples in our dataset - so we can ensure everything worked out so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9ce94-eb42-4a24-9a95-d92bd285ccbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "65a9ce94-eb42-4a24-9a95-d92bd285ccbd",
    "outputId": "eac55fd7-b193-4f80-9f4e-6d5d3b101e9c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(4):\n",
    "        ax = plt.subplot(2, 2, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3bc3a-2a2b-4f97-9da8-5097e5aa67ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dca3bc3a-2a2b-4f97-9da8-5097e5aa67ab",
    "outputId": "2b602ecd-0a76-4ba7-8530-5cb755f554a5"
   },
   "outputs": [],
   "source": [
    "# check dimensions of batch with multiple images\n",
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f541389a-6b54-463e-91d6-2e4b994a684f",
   "metadata": {
    "id": "f541389a-6b54-463e-91d6-2e4b994a684f"
   },
   "outputs": [],
   "source": [
    "# code to optimize the use of buffer when loading data\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4212d-4796-4ce4-9f41-3d16c0a36078",
   "metadata": {
    "id": "daf4212d-4796-4ce4-9f41-3d16c0a36078"
   },
   "outputs": [],
   "source": [
    "# pixel values will be between 0 and 1 afterwards\n",
    "normalization_layer = layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774c4f8-8573-47a4-85db-aa1c5407cdad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6774c4f8-8573-47a4-85db-aa1c5407cdad",
    "outputId": "bec19b9f-69aa-4a1d-a7e0-b3664e06660a"
   },
   "outputs": [],
   "source": [
    "# check if normalization worked\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0., 1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23936c-2e8b-4afe-98d1-832d11fcb83e",
   "metadata": {
    "id": "4b23936c-2e8b-4afe-98d1-832d11fcb83e"
   },
   "source": [
    "### Convolutional layer\n",
    "![image.png](attachment:a827f077-6caf-4faa-9727-70576b8477c6.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c327c-c1e4-481e-b18a-1d7a9e083137",
   "metadata": {
    "id": "a48c327c-c1e4-481e-b18a-1d7a9e083137"
   },
   "outputs": [],
   "source": [
    "# how many classes do we have?\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# create neural network\n",
    "model = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "    # here is our convolutional layer -> 8 Kernels, Kernel Size = 3x3, padding to keep dimension of image\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    # here is our convolutional layer -> 16 Kernels, Kernel Size = 3x3, padding to keep dimension of image\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    # reduce our image dimension here\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    # we flatten our images to get 1D data\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    # here we use two outputs - the higher one determines the predicted class\n",
    "    layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "H5oX_AhMWF4r"
   },
   "id": "H5oX_AhMWF4r",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "_ZYQuyEpWFhl"
   },
   "id": "_ZYQuyEpWFhl",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70702e2f-0f01-4ee6-8b68-d71a48f14e17",
   "metadata": {
    "id": "70702e2f-0f01-4ee6-8b68-d71a48f14e17"
   },
   "outputs": [],
   "source": [
    "# we want to minimize the loss - and use the adam optimizer to do this\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4e859-7380-4cc4-b0fd-e16ce19dc293",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33c4e859-7380-4cc4-b0fd-e16ce19dc293",
    "outputId": "7ffda011-358c-4d14-bd2a-40e3231a6c7d"
   },
   "outputs": [],
   "source": [
    "# how does our model look like\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4c0bc-36eb-4981-b6ac-43d199e11714",
   "metadata": {
    "id": "edc4c0bc-36eb-4981-b6ac-43d199e11714"
   },
   "source": [
    "### Fully connected layer\n",
    "![image](https://miro.medium.com/max/439/1*sVvC9YwPFD5RJ9xgxrYHPw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6b816-1024-40d6-9d5a-6a79f932647c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53b6b816-1024-40d6-9d5a-6a79f932647c",
    "outputId": "3c008b4b-5230-48b7-c068-ac89ad57179d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# lets train for 10 epochs\n",
    "epochs = 10\n",
    "\n",
    "# here the actual training happens\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb880c-8f28-4dfd-a09c-ffa4ec56eab8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "00bb880c-8f28-4dfd-a09c-ffa4ec56eab8",
    "outputId": "4c4a5c98-197e-492a-b569-4a46139d0f14"
   },
   "outputs": [],
   "source": [
    "# just some code to plot the training and validation accuracy and loss\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c346ea1-5f8a-4dc7-a34a-b1ee2b271b8c",
   "metadata": {
    "id": "9c346ea1-5f8a-4dc7-a34a-b1ee2b271b8c"
   },
   "source": [
    "Now we have the first predictions from our trained model. Let's have a look at the examples we have reserved for validation.\n",
    "\n",
    "The left value is the correct class, the right value is the class predicted by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e435a427-6c1d-46d4-87ca-8b42f794a3a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e435a427-6c1d-46d4-87ca-8b42f794a3a1",
    "outputId": "ef3732b0-116c-4ba0-c794-3a8d98b6807e"
   },
   "outputs": [],
   "source": [
    "for element, labels in val_ds:\n",
    "    predictions = model.predict(element)\n",
    "    batch_len = len(predictions)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for idx, (image, label, prediction) in enumerate(zip(element, labels, predictions)):\n",
    "        ax = plt.subplot(1, 4, idx + 1)\n",
    "        img = image.numpy().astype(\"uint8\")\n",
    "        plt.imshow(img)\n",
    "        pred_label = class_names[np.argmax(prediction)]\n",
    "        plt.title(f\"{class_names[label]} - {pred_label}\")\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d460a7-1576-4855-bf88-df6ce4f2d24c",
   "metadata": {
    "id": "89d460a7-1576-4855-bf88-df6ce4f2d24c"
   },
   "source": [
    "## Publikumsbefragung:\n",
    "### We observed some problems here\n",
    "\n",
    "\n",
    "#### How can we get better?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f368541-8215-4152-9a3f-0fad382d26c2",
   "metadata": {
    "id": "6f368541-8215-4152-9a3f-0fad382d26c2"
   },
   "source": [
    "## Improvement: Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7412b9-886d-453f-9648-0e557c66faf2",
   "metadata": {
    "id": "8d7412b9-886d-453f-9648-0e557c66faf2"
   },
   "outputs": [],
   "source": [
    "# here we augment the data\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    # we randomly flip them horizonatally\n",
    "    layers.RandomFlip(\n",
    "        \"horizontal\",\n",
    "        input_shape=(img_height,img_width, 3)),\n",
    "    # we randomly rotate them\n",
    "    layers.RandomRotation(0.1),\n",
    "    # we randomly zoom\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")\n",
    "\n",
    "def augment(img):\n",
    "    return data_augmentation(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52eca9e-1613-4713-ac5d-8b28c647aeff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 684
    },
    "id": "f52eca9e-1613-4713-ac5d-8b28c647aeff",
    "outputId": "60535193-d625-4b2e-f1c7-a0b209dab0df"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images_raw, _ in train_ds:\n",
    "    for i in range(9):\n",
    "        augmented_images = augment(images_raw)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686e4c6-433f-4f84-905c-7e3d6fbaf60c",
   "metadata": {
    "id": "c686e4c6-433f-4f84-905c-7e3d6fbaf60c"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # here we add the additional data augmentation layer:\n",
    "    data_augmentation,\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f148b3-9328-485f-b680-08d6d0810afc",
   "metadata": {
    "id": "64f148b3-9328-485f-b680-08d6d0810afc"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f259c1f-7c65-436f-b42e-5924c62b2999",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f259c1f-7c65-436f-b42e-5924c62b2999",
    "outputId": "c1085ae6-d417-4a00-b4f5-3e0a9cdf99f8"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e8c0a-f457-494d-be3f-3604c726c5fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "367e8c0a-f457-494d-be3f-3604c726c5fd",
    "outputId": "3416d084-5967-4878-e074-799f7d29b367"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# once again we train the model with some more epochs,\n",
    "#  because we do not fear overfitting so much with the augmentation\n",
    "epochs = 16\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80dc9f-87e9-4a98-95d5-11d90a024f02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "7b80dc9f-87e9-4a98-95d5-11d90a024f02",
    "outputId": "230195bb-c84b-4694-980e-3101026604a4"
   },
   "outputs": [],
   "source": [
    "# same plotting code again\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e95f2-66aa-4b93-ad3c-9865de52a3c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "782e95f2-66aa-4b93-ad3c-9865de52a3c2",
    "outputId": "202f73bf-a918-46ee-d37f-d1ccafaf4a43"
   },
   "outputs": [],
   "source": [
    "for element, labels in val_ds:\n",
    "    predictions = model.predict(element)\n",
    "    batch_len = len(predictions)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for idx, (image, label, prediction) in enumerate(zip(element, labels, predictions)):\n",
    "        ax = plt.subplot(1, 4, idx + 1)\n",
    "        img = image.numpy().astype(\"uint8\")\n",
    "        plt.imshow(img)\n",
    "        pred_label = class_names[np.argmax(prediction)]\n",
    "        plt.title(f\"{class_names[label]} - {pred_label}\")\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbfafd1-c1dc-4969-be22-c6c0dde7f43d",
   "metadata": {
    "id": "ebbfafd1-c1dc-4969-be22-c6c0dde7f43d"
   },
   "source": [
    "## Improvement: Dropout\n",
    "Another way to reduce overfitting might be \"dropout\": some neurons will be set to 0 during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f01fd4-1b2d-4947-b2f2-0b236d362863",
   "metadata": {
    "id": "c2f01fd4-1b2d-4947-b2f2-0b236d362863"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    # we add the dropout layer here\n",
    "    layers.Dropout(0.05),\n",
    "    layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac276e9b-5b5f-4646-9c06-637eab31624a",
   "metadata": {
    "id": "ac276e9b-5b5f-4646-9c06-637eab31624a"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9601c7e-5c7e-4fc0-bb61-7cb2a884492e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9601c7e-5c7e-4fc0-bb61-7cb2a884492e",
    "outputId": "e4224d30-5733-4472-dddd-0ebc1429fae0"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d55228-4417-4e91-9899-1f8f1a55052a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34d55228-4417-4e91-9899-1f8f1a55052a",
    "outputId": "65b56a3f-99dd-4bb6-d061-c4e2ac7fd624"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 12\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170efb96-1f71-4a76-bb3d-95ef5201ac22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "170efb96-1f71-4a76-bb3d-95ef5201ac22",
    "outputId": "1459684e-9bfe-4102-e21a-2c9af3fdc4d5"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472f967-9690-4f1b-a95f-eea11dc56b7b",
   "metadata": {
    "id": "a472f967-9690-4f1b-a95f-eea11dc56b7b"
   },
   "source": [
    "## Visualization\n",
    "In this area, we can visualize what the neural network predicts on our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b770a4-200c-47bf-b756-c0090d26df19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "90b770a4-200c-47bf-b756-c0090d26df19",
    "outputId": "139bcd0d-e73b-4b89-86e5-912f4abc5774"
   },
   "outputs": [],
   "source": [
    "for element, labels in val_ds:\n",
    "    predictions = model.predict(element)\n",
    "    batch_len = len(predictions)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for idx, (image, label, prediction) in enumerate(zip(element, labels, predictions)):\n",
    "        ax = plt.subplot(1, 4, idx + 1)\n",
    "        img = image.numpy().astype(\"uint8\")\n",
    "        plt.imshow(img)\n",
    "        pred_label = class_names[np.argmax(prediction)]\n",
    "        plt.title(f\"{class_names[label]} - {pred_label}\")\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import display, Javascript, Image\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode, b64encode\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "import io\n",
    "import html\n",
    "import time\n",
    "\n",
    "\n",
    "# function to convert the JavaScript object into an OpenCV image\n",
    "def js_to_image(js_reply):\n",
    "  \"\"\"\n",
    "  Params:\n",
    "          js_reply: JavaScript object containing image from webcam\n",
    "  Returns:\n",
    "          img: OpenCV BGR image\n",
    "  \"\"\"\n",
    "  # decode base64 image\n",
    "  image_bytes = b64decode(js_reply.split(',')[1])\n",
    "  # convert bytes to numpy array\n",
    "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "  # decode numpy array into OpenCV BGR image\n",
    "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
    "\n",
    "  return img\n",
    "\n",
    "\n",
    "\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: {width: { ideal: 640 },\n",
    "        height: { ideal: 640 }} });\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "\n",
    "  # get photo data\n",
    "  data = eval_js('takePhoto({})'.format(quality))\n",
    "  # get OpenCV format image\n",
    "  img = js_to_image(data)\n",
    "\n",
    "  cv2.imwrite(filename, img)\n",
    "\n",
    "  return filename"
   ],
   "metadata": {
    "id": "T2Z5_ZKbRRJ_"
   },
   "id": "T2Z5_ZKbRRJ_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test the model on your own examples"
   ],
   "metadata": {
    "id": "ClfMNNhoRpWv"
   },
   "id": "ClfMNNhoRpWv"
  },
  {
   "cell_type": "code",
   "source": [
    "#Execute this code block to save a picture from your webcam\n",
    "try:\n",
    "  file = take_photo('photo.jpg')\n",
    "\n",
    "\n",
    "\n",
    "  display(Image(file))\n",
    "except Exception as err:\n",
    "    # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "  # grant the page permission to access it.\n",
    "  print(str(err))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "JOR97TaURomZ",
    "outputId": "87e68c3d-c0be-4a21-ed5b-3df3aefcf411"
   },
   "id": "JOR97TaURomZ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "img = PIL.Image.open('photo.jpg')\n",
    "img = np.array(img.resize((img_width, img_height)))\n",
    "img = np.reshape(img[:,:,:3], (1, img_width, img_height, 3))\n",
    "softmax_layer = layers.Softmax()\n",
    "prediction = softmax_layer(model.predict(img)).numpy()[0]\n",
    "pred_class = np.argmax(prediction)\n",
    "print(\"Prediction: \",class_names[pred_class], \" Probability: \", prediction[pred_class])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "haYVQF63ZXux",
    "outputId": "85e00def-ff7f-4e1d-a479-5a627d5332b0"
   },
   "id": "haYVQF63ZXux",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cef44a03-76c6-46bc-813e-97cc7ef7e4a0",
   "metadata": {
    "id": "cef44a03-76c6-46bc-813e-97cc7ef7e4a0"
   },
   "source": [
    "# Thank you!\n",
    "If you read this you really deserve a big THANK YOU!\n",
    "\n",
    "THANK YOU for participating, you are awesome!\n",
    "Now it's up to you! Use this notebook as a template and train you own network!\n",
    "Just exchange the folders with your training data, you can even add more classes or whatever you like!\n",
    "Be creative! Have FUN!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6768ba-db1b-48a5-8500-67255f1975b3",
   "metadata": {
    "id": "dc6768ba-db1b-48a5-8500-67255f1975b3"
   },
   "source": [
    "## Contact\n",
    "If you have any questions, don't hesitate to write me on\n",
    "* linkedin: https://www.linkedin.com/in/paul-puntschart-279506a2/\n",
    "* email:\n",
    "* paul.puntschart@cloudflight.io\n",
    "* marcel.brunnbauer@cloudflight.io\n",
    "* john.uroko@cloudflight.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88f3de-0350-4ba3-b749-242e62e4774c",
   "metadata": {
    "id": "4a88f3de-0350-4ba3-b749-242e62e4774c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
